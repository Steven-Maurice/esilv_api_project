<html><body><h1><a href='http://127.0.0.1:5000/articles/artid=2404.04254v1'>Watermark-based Detection and Attribution of AI-Generated Content</a></h1><p>Authors: ['Zhengyuan Jiang', 'Moyang Guo', 'Yuepeng Hu', 'Neil Zhenqiang Gong']</p><p>Published: 2024-04-05</p><p>  Several companies--such as Google, Microsoft, and OpenAI--have deployed
techniques to watermark AI-generated content to enable proactive detection.
However, existing literature mainly focuses on user-agnostic detection.
Attribution aims to further trace back the user of a generative-AI service who
generated a given content detected as AI-generated. Despite its growing
importance, attribution is largely unexplored. In this work, we aim to bridge
this gap by providing the first systematic study on watermark-based, user-aware
detection and attribution of AI-generated content. Specifically, we
theoretically study the detection and attribution performance via rigorous
probabilistic analysis. Moreover, we develop an efficient algorithm to select
watermarks for the users to enhance attribution performance. Both our
theoretical and empirical results show that watermark-based detection and
attribution inherit the accuracy and (non-)robustness properties of the
watermarking method.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04254v1</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.04254v1'>ID : 2404.04254v1</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2404.04253v1'>Growing Q-Networks: Solving Continuous Control Tasks with Adaptive
  Control Resolution</a></h1><p>Authors: ['Tim Seyde', 'Peter Werner', 'Wilko Schwarting', 'Markus Wulfmeier', 'Daniela Rus']</p><p>Published: 2024-04-05</p><p>  Recent reinforcement learning approaches have shown surprisingly strong
capabilities of bang-bang policies for solving continuous control benchmarks.
The underlying coarse action space discretizations often yield favourable
exploration characteristics while final performance does not visibly suffer in
the absence of action penalization in line with optimal control theory. In
robotics applications, smooth control signals are commonly preferred to reduce
system wear and energy efficiency, but action costs can be detrimental to
exploration during early training. In this work, we aim to bridge this
performance gap by growing discrete action spaces from coarse to fine control
resolution, taking advantage of recent results in decoupled Q-learning to scale
our approach to high-dimensional action spaces up to dim(A) = 38. Our work
indicates that an adaptive control resolution in combination with value
decomposition yields simple critic-only algorithms that yield surprisingly
strong performance on continuous control tasks.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04253v1</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.04253v1'>ID : 2404.04253v1</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2404.04251v1'>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt
  Coherence Metrics with T2IScoreScore (TS2)</a></h1><p>Authors: ['Michael Saxon', 'Fatima Jahara', 'Mahsa Khoshnoodi', 'Yujie Lu', 'Aditya Sharma', 'William Yang Wang']</p><p>Published: 2024-04-05</p><p>  With advances in the quality of text-to-image (T2I) models has come interest
in benchmarking their prompt faithfulness-the semantic coherence of generated
images to the prompts they were conditioned on. A variety of T2I faithfulness
metrics have been proposed, leveraging advances in cross-modal embeddings and
vision-language models (VLMs). However, these metrics are not rigorously
compared and benchmarked, instead presented against few weak baselines by
correlation to human Likert scores over a set of easy-to-discriminate images.
  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs
containing a prompt and a set increasingly erroneous images. These allow us to
rigorously judge whether a given prompt faithfulness metric can correctly order
images with respect to their objective error count and significantly
discriminate between different error nodes, using meta-metric scores derived
from established statistical tests. Surprisingly, we find that the
state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we
tested fail to significantly outperform simple feature-based metrics like
CLIPScore, particularly on a hard subset of naturally-occurring T2I model
errors. TS2 will enable the development of better T2I prompt faithfulness
metrics through more rigorous comparison of their conformity to expected
orderings and separations under objective criteria.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04251v1</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.04251v1'>ID : 2404.04251v1</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2404.04243v1'>Identity Decoupling for Multi-Subject Personalization of Text-to-Image
  Models</a></h1><p>Authors: ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang']</p><p>Published: 2024-04-05</p><p>  Text-to-image diffusion models have shown remarkable success in generating a
personalized subject based on a few reference images. However, current methods
struggle with handling multiple subjects simultaneously, often resulting in
mixed identities with combined attributes from different subjects. In this
work, we present MuDI, a novel framework that enables multi-subject
personalization by effectively decoupling identities from multiple subjects.
Our main idea is to utilize segmented subjects generated by the Segment
Anything Model for both training and inference, as a form of data augmentation
for training and initialization for the generation process. Our experiments
demonstrate that MuDI can produce high-quality personalized images without
identity mixing, even for highly similar subjects as shown in Figure 1. In
human evaluation, MuDI shows twice as many successes for personalizing multiple
subjects without identity mixing over existing baselines and is preferred over
70% compared to the strongest baseline. More results are available at
https://mudi-t2i.github.io/.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04243v1</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.04243v1'>ID : 2404.04243v1</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2404.04242v1'>Physical Property Understanding from Language-Embedded Feature Fields</a></h1><p>Authors: ['Albert J. Zhai', 'Yuan Shen', 'Emily Y. Chen', 'Gloria X. Wang', 'Xinlei Wang', 'Sheng Wang', 'Kaiyu Guan', 'Shenlong Wang']</p><p>Published: 2024-04-05</p><p>  Can computers perceive the physical properties of objects solely through
vision? Research in cognitive science and vision science has shown that humans
excel at identifying materials and estimating their physical properties based
purely on visual appearance. In this paper, we present a novel approach for
dense prediction of the physical properties of objects using a collection of
images. Inspired by how humans reason about physics through vision, we leverage
large language models to propose candidate materials for each object. We then
construct a language-embedded point cloud and estimate the physical properties
of each 3D point using a zero-shot kernel regression approach. Our method is
accurate, annotation-free, and applicable to any object in the open world.
Experiments demonstrate the effectiveness of the proposed approach in various
physical property reasoning tasks, such as estimating the mass of common
objects, as well as other properties like friction and hardness.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04242v1</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.04242v1'>ID : 2404.04242v1</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2311.08577v3'>Finding AI-Generated Faces in the Wild</a></h1><p>Authors: ['Gonzalo J. Aniano Porcile', 'Jack Gindi', 'Shivansh Mundra', 'James R. Verbus', 'Hany Farid']</p><p>Published: 2023-11-14</p><p>  AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2311.08577v3</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2311.08577v3'>ID : 2311.08577v3</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2404.04234v1'>player2vec: A Language Modeling Approach to Understand Player Behavior
  in Games</a></h1><p>Authors: ['Tianze Wang', 'Maryam Honari-Jahromi', 'Styliani Katsarou', 'Olga Mikheeva', 'Theodoros Panagiotakopoulos', 'Sahar Asadi', 'Oleg Smirnov']</p><p>Published: 2024-04-05</p><p>  Methods for learning latent user representations from historical behavior
logs have gained traction for recommendation tasks in e-commerce, content
streaming, and other settings. However, this area still remains relatively
underexplored in video and mobile gaming contexts. In this work, we present a
novel method for overcoming this limitation by extending a long-range
Transformer model from the natural language processing domain to player
behavior data. We discuss specifics of behavior tracking in games and propose
preprocessing and tokenization approaches by viewing in-game events in an
analogous way to words in sentences, thus enabling learning player
representations in a self-supervised manner in the absence of ground-truth
annotations. We experimentally demonstrate the efficacy of the proposed
approach in fitting the distribution of behavior events by evaluating intrinsic
language modeling metrics. Furthermore, we qualitatively analyze the emerging
structure of the learned embedding space and show its value for generating
insights into behavior patterns to inform downstream applications.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04234v1</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.04234v1'>ID : 2404.04234v1</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2404.03635v2'>WorDepth: Variational Language Prior for Monocular Depth Estimation</a></h1><p>Authors: ['Ziyao Zeng', 'Daniel Wang', 'Fengyu Yang', 'Hyoungseob Park', 'Yangchao Wu', 'Stefano Soatto', 'Byung-Woo Hong', 'Dong Lao', 'Alex Wong']</p><p>Published: 2024-04-04</p><p>  Three-dimensional (3D) reconstruction from a single image is an ill-posed
problem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text
description(s) is similarly ill-posed, i.e. spatial arrangements of objects
described. We investigate the question of whether two inherently ambiguous
modalities can be used in conjunction to produce metric-scaled reconstructions.
To test this, we focus on monocular depth estimation, the problem of predicting
a dense depth map from a single image, but with an additional text caption
describing the scene. To this end, we begin by encoding the text caption as a
mean and standard deviation; using a variational framework, we learn the
distribution of the plausible metric reconstructions of 3D scenes corresponding
to the text captions as a prior. To "select" a specific reconstruction or depth
map, we encode the given image through a conditional sampler that samples from
the latent space of the variational text encoder, which is then decoded to the
output depth map. Our approach is trained alternatingly between the text and
image branches: in one optimization step, we predict the mean and standard
deviation from the text description and sample from a standard Gaussian, and in
the other, we sample using a (image) conditional sampler. Once trained, we
directly predict depth from the encoded text using the conditional sampler. We
demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where
we show that language can consistently improve performance in both.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.03635v2</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.03635v2'>ID : 2404.03635v2</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2311.09559v2'>Prompt-based Pseudo-labeling Strategy for Sample-Efficient
  Semi-Supervised Extractive Summarization</a></h1><p>Authors: ['Gaurav Sahu', 'Olga Vechtomova', 'Issam H. Laradji']</p><p>Published: 2023-11-16</p><p>  Semi-supervised learning (SSL) is a widely used technique in scenarios where
labeled data is scarce and unlabeled data is abundant. While SSL is popular for
image and text classification, it is relatively underexplored for the task of
extractive text summarization. Standard SSL methods follow a teacher-student
paradigm to first train a classification model and then use the classifier's
confidence values to select pseudo-labels for the subsequent training cycle;
however, such classifiers are not suitable to measure the accuracy of
pseudo-labels as they lack specific tuning for evaluation, which leads to
confidence values that fail to capture the semantics and correctness of the
generated summary. To address this problem, we propose a prompt-based
pseudo-labeling strategy with LLMs that picks unlabeled examples with more
accurate pseudo-labels than using just the classifier's probability outputs.
Our approach also includes a relabeling mechanism that improves the quality of
pseudo-labels. We evaluate our method on three text summarization datasets:
TweetSumm, WikiHow, and ArXiv/PubMed. We empirically show that a
prompting-based LLM that scores and generates pseudo-labels outperforms
existing SSL methods on ROUGE-1, ROUGE-2, and ROUGE-L scores on all the
datasets. Furthermore, our method achieves competitive G-Eval scores
(evaluation with GPT-4) as a fully supervised method that uses 100% of the
labeled data with only 16.67% of the labeled data.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2311.09559v2</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2311.09559v2'>ID : 2311.09559v2</a>
<h1><a href='http://127.0.0.1:5000/articles/artid=2404.04220v1'>Multi-modal perception for soft robotic interactions using generative
  models</a></h1><p>Authors: ['Enrico Donato', 'Egidio Falotico', 'Thomas George Thuruthel']</p><p>Published: 2024-04-05</p><p>  Perception is essential for the active interaction of physical agents with
the external environment. The integration of multiple sensory modalities, such
as touch and vision, enhances this perceptual process, creating a more
comprehensive and robust understanding of the world. Such fusion is
particularly useful for highly deformable bodies such as soft robots.
Developing a compact, yet comprehensive state representation from multi-sensory
inputs can pave the way for the development of complex control strategies. This
paper introduces a perception model that harmonizes data from diverse
modalities to build a holistic state representation and assimilate essential
information. The model relies on the causality between sensory input and
robotic actions, employing a generative model to efficiently compress fused
information and predict the next observation. We present, for the first time, a
study on how touch can be predicted from vision and proprioception on soft
robots, the importance of the cross-modal generation and why this is essential
for soft robotic interactions in unstructured environments.
</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04220v1</a>
</br><a href='http://127.0.0.1:5000/ml/artid=2404.04220v1'>ID : 2404.04220v1</a>
</body></html></body></html>
<br><a href='/'>Return to Home</a>