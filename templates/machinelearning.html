<html><body><h1>Watermark-based Detection and Attribution of AI-Generated Content</h1><p>Authors: ['Zhengyuan Jiang', 'Moyang Guo', 'Yuepeng Hu', 'Neil Zhenqiang Gong']</p><p>Published: 2024-04-05</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04254v1</a><p>Sentiment: 4 stars</p><h1>Growing Q-Networks: Solving Continuous Control Tasks with Adaptive
  Control Resolution</h1><p>Authors: ['Tim Seyde', 'Peter Werner', 'Wilko Schwarting', 'Markus Wulfmeier', 'Daniela Rus']</p><p>Published: 2024-04-05</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04253v1</a><p>Sentiment: 4 stars</p><h1>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt
  Coherence Metrics with T2IScoreScore (TS2)</h1><p>Authors: ['Michael Saxon', 'Fatima Jahara', 'Mahsa Khoshnoodi', 'Yujie Lu', 'Aditya Sharma', 'William Yang Wang']</p><p>Published: 2024-04-05</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04251v1</a><p>Sentiment: 4 stars</p><h1>Identity Decoupling for Multi-Subject Personalization of Text-to-Image
  Models</h1><p>Authors: ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang']</p><p>Published: 2024-04-05</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04243v1</a><p>Sentiment: 4 stars</p><h1>Physical Property Understanding from Language-Embedded Feature Fields</h1><p>Authors: ['Albert J. Zhai', 'Yuan Shen', 'Emily Y. Chen', 'Gloria X. Wang', 'Xinlei Wang', 'Sheng Wang', 'Kaiyu Guan', 'Shenlong Wang']</p><p>Published: 2024-04-05</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04242v1</a><p>Sentiment: 4 stars</p><h1>Finding AI-Generated Faces in the Wild</h1><p>Authors: ['Gonzalo J. Aniano Porcile', 'Jack Gindi', 'Shivansh Mundra', 'James R. Verbus', 'Hany Farid']</p><p>Published: 2023-11-14</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2311.08577v3</a><p>Sentiment: 4 stars</p><h1>player2vec: A Language Modeling Approach to Understand Player Behavior
  in Games</h1><p>Authors: ['Tianze Wang', 'Maryam Honari-Jahromi', 'Styliani Katsarou', 'Olga Mikheeva', 'Theodoros Panagiotakopoulos', 'Sahar Asadi', 'Oleg Smirnov']</p><p>Published: 2024-04-05</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04234v1</a><p>Sentiment: 3 stars</p><h1>WorDepth: Variational Language Prior for Monocular Depth Estimation</h1><p>Authors: ['Ziyao Zeng', 'Daniel Wang', 'Fengyu Yang', 'Hyoungseob Park', 'Yangchao Wu', 'Stefano Soatto', 'Byung-Woo Hong', 'Dong Lao', 'Alex Wong']</p><p>Published: 2024-04-04</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.03635v2</a><p>Sentiment: 2 stars</p><h1>Prompt-based Pseudo-labeling Strategy for Sample-Efficient
  Semi-Supervised Extractive Summarization</h1><p>Authors: ['Gaurav Sahu', 'Olga Vechtomova', 'Issam H. Laradji']</p><p>Published: 2023-11-16</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2311.09559v2</a><p>Sentiment: 2 stars</p><h1>Multi-modal perception for soft robotic interactions using generative
  models</h1><p>Authors: ['Enrico Donato', 'Egidio Falotico', 'Thomas George Thuruthel']</p><p>Published: 2024-04-05</p><a href=('Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models', '  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n', ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'http://arxiv.org/abs/2404.04243v1', '2024-04-05T17:45:22Z')>http://arxiv.org/abs/2404.04220v1</a><p>Sentiment: 5 stars</p></body></html>
<br><a href='/'>Return to Home</a>