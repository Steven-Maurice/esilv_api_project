# -*- coding: utf-8 -*-
"""api.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c9Jy3X1FWeok2kbGnFyG2T8-lIVQz7jJ
""" 
import requests
from bs4 import BeautifulSoup

app = Flask(__name__)

def get_article_content(article_url):
    response = requests.get(article_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        
        date_published = soup.find("div", class_="entry-meta").find("span").get_text(strip=True)
        
        author_description = soup.find("span", class_="author-name")
        author_description_text = author_description.get_text(strip=True) if author_description else "Auteur non disponible"
        
        content_div = soup.find("div", class_="entry-content")
        if content_div:
            paragraphs = content_div.find_all("p")
            content_text = ' '.join(p.get_text(strip=True) for p in paragraphs)
            if content_text: 
                return {
                    "date_published": date_published,
                    "author_description": author_description_text,
                    "content": content_text
                }
        return None
    else:
        return None

def get_articles_from_page(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        articles = []
        for article_header in soup.find_all("h3", class_="entry-title"):
            if len(articles) >= 10:  
                break
            article_link = article_header.find("a")
            if article_link:
                title = article_link.text.strip()
                article_url = article_link['href']
                article_content = get_article_content(article_url)
                if article_content:  
                    articles.append({
                        "title": title,
                        "url": article_url,
                        "date_published": article_content["date_published"],
                        "author_description": article_content["author_description"],
                        "content": article_content["content"]
                    })
        return articles
    else:
        print("Échec de la récupération de la page.")
        return []

url = "https://www.actuia.com/actualite/"
articles = get_articles_from_page(url)
for article in articles:
    print("Titre:", article["title"])
    print("Date de publication:", article["date_published"])
    print("URL:", article["url"])
    print("Auteur:", article["author_description"])
    print('Contenu:', article['content'])

@app.route('/get_data', methods=['GET'])
def get_data():
    url = "https://www.actuia.com/actualite/"
    articles = get_articles_from_page(url, 5)
    return jsonify(articles)

@app.route('/articles', methods=['GET'])
def articles():
    url = "https://www.actuia.com/actualite/"
    articles = get_articles_from_page(url, 5)
    articles_info = [{"number": idx+1, "title": article["title"], "date_published": article["date_published"], "url": article["url"]} for idx, article in enumerate(articles)]
    return jsonify(articles_info)

@app.route('/article/<int:number>', methods=['GET'])
def article(number):
    url = "https://www.actuia.com/actualite/"
    articles = get_articles_from_page(url)
    if 1 <= number <= len(articles):
        article = articles[number-1]
        return jsonify({"title": article["title"], "content": article["content"]})
    else:
        return jsonify({"error": "Article not found"}), 404

if __name__ == '__main__':
    app.run(debug=True)
