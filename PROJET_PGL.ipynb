{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTATION DES MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (20231228)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Requirement already satisfied: textblob in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documentation of the API : \n",
    "# https://info.arxiv.org/help/api/user-manual.html#_calling_the_api\n",
    "!pip install requests\n",
    "import requests #\n",
    "\n",
    "from xml.etree import ElementTree #pip install xml.etree # Pour lire les données extraites par l'api arXiv\n",
    "import datetime as dt #pip install datetime #Pour convertir les dates de publicaiton de str en datetime pour les ranger par ordre chronologique\n",
    "\n",
    "# 3 modules pour l'extraction du texte du pdf, manque tjrs les images...\n",
    "import io\n",
    "!pip install pdfminer.six\n",
    "import pdfminer #pip install pdfminer\n",
    "from pdfminer.high_level import extract_text  #pip install pdfminer.six\n",
    "\n",
    "# pour analyse de sentiment\n",
    "!pip install textblob\n",
    "from textblob import TextBlob # #pip install textblob\n",
    "\n",
    "# pour analyse mots clés\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de l'API, présentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response\n",
    "!pip install matplotlib\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Présentation de notre API'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appel à l'API et stockage des données dans une liste pour éviter de faire plein de fois le même appel pour la même requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callAPI(urlreq, param):\n",
    "    arxiv_url = urlreq\n",
    "    research_type = param\n",
    "    rep = requests.get(arxiv_url, params=research_type)\n",
    "    root = ElementTree.fromstring(rep.content)\n",
    "    return root\n",
    "\n",
    "def articlesAPI():\n",
    "    articles = []\n",
    "    # Ici on défini notre url de requête et nos paramètres pour faire appel à l'api\n",
    "    # On va chercher partout où le titre contient AI, et on garde 5 artciles comme dit la consigne\n",
    "    root = callAPI(\"http://export.arxiv.org/api/query?\",{\"search_query\": \"all:ai\", \"max_results\" : 5})\n",
    "    # On regarde élément par élément dans notre recherche : le titre/l'abstract/le nom de l'auteur/\n",
    "    #la date de publication/le lien pour accéder à l\"article/l'identifiant associé\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        abs = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        auteur = entry.find('{http://www.w3.org/2005/Atom}author').find('{http://www.w3.org/2005/Atom}name').text\n",
    "        published = entry.find('{http://www.w3.org/2005/Atom}published').text[0:10]\n",
    "        link = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
    "        id = link[21:]\n",
    "        urlpdf = f\"http://arxiv.org/pdf/{id}\"\n",
    "        com = entry.find('{http://arxiv.org/schemas/atom}comment')\n",
    "        com = com.text if com is not None else 'No comments'\n",
    "        prim_cat = entry.find('{http://arxiv.org/schemas/atom}primary_category')\n",
    "        prim_cat = prim_cat.attrib['term'] if prim_cat is not None else 'No primary category'\n",
    "\n",
    "        articles.append([id, link, auteur, title, published, urlpdf, abs, prim_cat, com])\n",
    "\n",
    "    for article in articles: # pour trier les artciles dans l'ordre chronologik ; pas trouvé comment faire avec l'api directement\n",
    "        article[4] = dt.datetime.strptime(article[4], '%Y-%m-%d')\n",
    "    articles.sort(key=lambda x: x[4], reverse = True)\n",
    "\n",
    "    for i in articles :\n",
    "        if i[7] == \"cs.AI\":\n",
    "          i[7] = \"IA\"\n",
    "        if i[7] == \"cs.CR\":\n",
    "          i[7] = \"Cryptographie\"\n",
    "        if i[7] == \"cs.CY\":\n",
    "          i[7] = \"Cybernétique\"\n",
    "    return articles\n",
    "\n",
    "larticles = articlesAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ère route : get_data\n",
    "\n",
    "récupère 5 liens d'articles et les range par ordre chronologique de sortie, puis les affiche\n",
    "\n",
    "AMODIF, un peu nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response 239 bytes [200 OK]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@app.route('/get_data')\n",
    "def get_data():\n",
    "    liste = larticles\n",
    "    a = [i[7] +\" : \"+ i[1] for i in liste]\n",
    "    html = \"<br>\".join(a)\n",
    "    return Response(html, mimetype='text/html')\n",
    "\n",
    "get_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ème route : Affiche les informations relatives aux 5 articles précédents, id, lien, auteur, titre, date de publi, lien pdf, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/articles')\n",
    "def articles():\n",
    "    art = larticles \n",
    "    commentaire = \"Pour voir le contenu d'un article en particulier : modifier le chemin, /articles devient article/ID où ID est l'identifiant de l'article choisi\"\n",
    "    html = commentaire +  \"<br>\"+ \"<br>\".join([f\"ID: {a[0]}<br>Link : {a[1]}<br>Authors : {a[2]}<br>Title : {a[3]}<br>Published : {a[4]}<br>PDF : {a[5]}<br>Summary : {a[6]}<br>catégorie : {a[7]}<br>Commentaires : {a[8]}<br>\" for a in art])\n",
    "    return Response(html, mimetype='text/html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ème route : Affiche le lien pdf et son contenu d'un article identifié par son id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTEXTformPDF(id):\n",
    "    urlpdf = f\"http://arxiv.org/pdf/{id}\"\n",
    "    response = requests.get(urlpdf)\n",
    "    \n",
    "    text = (extract_text(io.BytesIO(response.content)), urlpdf)\n",
    "    return text\n",
    "\n",
    "@app.route('/article/<id>')\n",
    "\n",
    "def article_content(id):\n",
    "\n",
    "    text = extractTEXTformPDF(id)\n",
    "    html_content = \"Lien pour accéder au pdf direct : \" + text[1] +'<br>'+'<br>'+'<br>' + text[0].replace('\\n', '<br>')\n",
    "\n",
    "    return Response(html_content, mimetype='text/html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/ml/sentiment/<id>')\n",
    "def analyze_sentiment_article(id):\n",
    "\n",
    "    text = extractTEXTformPDF(id)\n",
    "\n",
    "    blob = TextBlob(text[0])\n",
    "    sentiment = blob.sentiment\n",
    "    strpolarity = \"polarity in [-1,1] ; avec <br> -1 : sentiment négatif <br> 0 : sentiment neutre <br> 1 sentiment positif \"\n",
    "    strsubj = \"subjectivity in [0,1] ; avec <br> 0 : ocnmplétement objectif <br> 1 : complétement subjectif \"\n",
    "\n",
    "    html_content = strpolarity + '<br><br>' +  strsubj + '<br><br><br> polarity : ' + str(sentiment.polarity) + '<br> subjectivity : ' + str(sentiment.subjectivity)\n",
    "\n",
    "    return Response(html_content, mimetype='text/html')\n",
    "\n",
    "def get_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "    freq_dist = FreqDist(keywords)\n",
    "    return freq_dist.most_common(10)\n",
    "\n",
    "@app.route('/ml/keywords/<id>')\n",
    "def keywords_article(id):\n",
    "    text = extractTEXTformPDF(id)[0]\n",
    "    keywords = get_keywords(text)\n",
    "    \n",
    "    # Convertir la liste des mots-clés en HTML\n",
    "    html_keywords = ''.join([f\"<li>{word[0]}: {word[1]}</li>\" for word in keywords])\n",
    "    html_content = f\"<ul>{html_keywords}</ul>\"\n",
    "\n",
    "    return Response(html_content, mimetype='text/html')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wordcloud) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->wordcloud) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install wordcloud\n",
    "\n",
    "def get_keywords2(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "    return keywords  \n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import send_file\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "@app.route('/ml/wordcloud/<id>')\n",
    "def wordcloud_article_direct(id):\n",
    "    text = extractTEXTformPDF(id)[0]\n",
    "    keywords = get_keywords2(text)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(keywords))\n",
    "    \n",
    "\n",
    "    img = BytesIO()\n",
    "    wordcloud.to_image().save(img, 'PNG')\n",
    "    img.seek(0) \n",
    "    \n",
    "   \n",
    "    return send_file(img, mimetype='image/png', as_attachment=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [07/Apr/2024 17:17:36] \"GET /ml/wordcloud/2305.15922v1 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# faire tourner le serveur\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
