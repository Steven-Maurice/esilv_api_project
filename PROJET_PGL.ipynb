{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTATION DES MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation of the API : \n",
    "# https://info.arxiv.org/help/api/user-manual.html#_calling_the_api\n",
    "\n",
    "import requests #pip install requests\n",
    "from xml.etree import ElementTree #pip install xml.etree # Pour lire les données extraites par l'api arXiv\n",
    "import datetime as dt #pip install datetime #Pour convertir les dates de publicaiton de str en datetime pour les ranger par ordre chronologique\n",
    "\n",
    "# 3 modules pour l'extraction du texte du pdf, manque tjrs les images...\n",
    "import io\n",
    "import pdfminer #pip install pdfminer\n",
    "from pdfminer.high_level import extract_text  #pip install pdfminer.six\n",
    "\n",
    "# pour analyse de sentiment\n",
    "from textblob import TextBlob # #pip install textblob\n",
    "\n",
    "# pour analyse mots clés\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import spacy #pip install spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de l'API, présentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, Response\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Bienvenu sur notre API(source arXiv)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appel à l'API et stockage des données dans une liste pour éviter de faire plein de fois le même appel pour la même requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callAPI(urlreq, param):\n",
    "    arxiv_url = urlreq\n",
    "    research_type = param\n",
    "    rep = requests.get(arxiv_url, params=research_type)\n",
    "    root = ElementTree.fromstring(rep.content)\n",
    "    return root\n",
    "\n",
    "\n",
    "def articlesAPI():\n",
    "    articles = []\n",
    "    # Ici on défini notre url de requête et nos paramètres pour faire appel à l'api\n",
    "    # On va chercher partout où le titre contient AI, et on garde 5 artciles comme dit la consigne\n",
    "    #root = callAPI(\"http://export.arxiv.org/api/query?\",{\"search_query\": \"all:ai\", \"start\" : 0, \"max_results\" : 20}) si on voulait 20 resultats\n",
    "    root = callAPI(\"http://export.arxiv.org/api/query?\",{\"search_query\": \"all:ai\", \"max_results\" : 5})\n",
    "    # On regarde élément par élément dans notre recherche : le titre/l'abstract/le nom de l'auteur/\n",
    "    #la date de publication/le lien pour accéder à l\"article/l'identifiant associé\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        abs = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        auteur = entry.find('{http://www.w3.org/2005/Atom}author').find('{http://www.w3.org/2005/Atom}name').text\n",
    "        published = entry.find('{http://www.w3.org/2005/Atom}published').text[0:10]\n",
    "        link = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
    "        id = link[21:]\n",
    "        urlpdf = f\"http://arxiv.org/pdf/{id}\"\n",
    "        com = entry.find('{http://arxiv.org/schemas/atom}comment')\n",
    "        com = com.text if com is not None else 'No comments'\n",
    "        prim_cat = entry.find('{http://arxiv.org/schemas/atom}primary_category')\n",
    "        prim_cat = prim_cat.attrib['term'] if prim_cat is not None else 'No primary category'\n",
    "\n",
    "        articles.append([id, link, auteur, title, published, urlpdf, abs, prim_cat, com])\n",
    "\n",
    "    for article in articles: # pour trier les artciles dans l'ordre chronologik ; pas trouvé comment faire avec l'api directement\n",
    "        article[4] = dt.datetime.strptime(article[4], '%Y-%m-%d')\n",
    "    articles.sort(key=lambda x: x[4], reverse = True)\n",
    "\n",
    "    for i in articles :\n",
    "        if i[7] == \"cs.AI\":\n",
    "          i[7] = \"IA\"\n",
    "        if i[7] == \"cs.CR\":\n",
    "          i[7] = \"Cryptographie\"\n",
    "        if i[7] == \"cs.CY\":\n",
    "          i[7] = \"Cybernétique\"\n",
    "    return articles\n",
    "\n",
    "larticles = articlesAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ère route : get_data\n",
    "\n",
    "récupère 5 liens d'articles et les range par ordre chronologique de sortie, puis les affiche\n",
    "\n",
    "AMODIF, un peu nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response 239 bytes [200 OK]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@app.route('/get_data')\n",
    "def get_data():\n",
    "    liste = larticles\n",
    "    a = [i[7] +\" : \"+ i[1] for i in liste]\n",
    "    html = \"<br>\".join(a)\n",
    "    return Response(html, mimetype='text/html')\n",
    "\n",
    "get_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ème route : Affiche les informations relatives aux 5 articles précédents, id, lien, auteur, titre, date de publi, lien pdf, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/articles')\n",
    "def articles():\n",
    "    art = larticles \n",
    "    commentaire = \"Pour voir le contenu d'un article en particulier : modifier le chemin, /articles devient article/ID où ID est l'identifiant de l'article choisi\"\n",
    "    html = commentaire +  \"<br>\"+ \"<br>\".join([f\"ID: {a[0]}<br>Link : {a[1]}<br>Authors : {a[2]}<br>Title : {a[3]}<br>Published : {a[4]}<br>PDF : {a[5]}<br>Summary : {a[6]}<br>catégorie : {a[7]}<br>Commentaires : {a[8]}<br>\" for a in art])\n",
    "    return Response(html, mimetype='text/html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ème route : Affiche le lien pdf et son contenu d'un article identifié par son id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTEXTformPDF(id):\n",
    "    urlpdf = f\"http://arxiv.org/pdf/{id}\"\n",
    "    response = requests.get(urlpdf)\n",
    "    \n",
    "    text = (extract_text(io.BytesIO(response.content)), urlpdf)\n",
    "    return text\n",
    "\n",
    "@app.route('/article/<id>')\n",
    "\n",
    "def article_content(id):\n",
    "\n",
    "    text = extractTEXTformPDF(id)\n",
    "    html_content = \"Lien pour accéder au pdf direct : \" + text[1] +'<br>'+'<br>'+'<br>' + text[0].replace('\\n', '<br>')\n",
    "\n",
    "    return Response(html_content, mimetype='text/html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/ml/<id>')\n",
    "def analyze_sentiment_article(id):\n",
    "\n",
    "    text = extractTEXTformPDF(id)\n",
    "\n",
    "    blob = TextBlob(text[0])\n",
    "    sentiment = blob.sentiment\n",
    "    strpolarity = \"polarity in [-1,1] ; avec <br> -1 : sentiment négatif <br> 0 : sentiment neutre <br> 1 sentiment positif \"\n",
    "    strsubj = \"subjectivity in [0,1] ; avec <br> 0 : ocnmplétement objectif <br> 1 : complétement subjectif \"\n",
    "\n",
    "    html_content = strpolarity + '<br><br>' +  strsubj + '<br><br><br> polarity : ' + str(sentiment.polarity) + '<br> subjectivity : ' + str(sentiment.subjectivity)\n",
    "\n",
    "    return Response(html_content, mimetype='text/html')\n",
    "\n",
    "def get_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "    freq_dist = FreqDist(keywords)\n",
    "    return freq_dist.most_common(10)\n",
    "\n",
    "@app.route('/ml/keywords/<id>')\n",
    "def keywords_article(id):\n",
    "    text = extractTEXTformPDF(id)[0]\n",
    "    keywords = get_keywords(text)\n",
    "    \n",
    "    # Convertir la liste des mots-clés en HTML\n",
    "    html_keywords = ''.join([f\"<li>{word[0]}: {word[1]}</li>\" for word in keywords])\n",
    "    html_content = f\"<ul>{html_keywords}</ul>\"\n",
    "\n",
    "    return Response(html_content, mimetype='text/html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_summarize(text, ratio=0.2):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentence_scores = {}\n",
    "    for sentence in doc.sents:\n",
    "        sentence_scores[sentence] = sum(token.vector for token in sentence if not token.is_stop).mean()\n",
    "\n",
    "    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
    "    selected_sentences = sorted_sentences[:int(len(sorted_sentences) * ratio)]\n",
    "\n",
    "    summary = ' '.join(str(sentence) for sentence in selected_sentences)\n",
    "\n",
    "    return summary\n",
    "\n",
    "@app.route('/ml/sum/<id>')\n",
    "def auto_summary_article(id):\n",
    "    text = extractTEXTformPDF(id)[0]\n",
    "    summary = auto_summarize(text)\n",
    "    html_content = f\"<h2>Résumé de l'article {id}</h2>\"\n",
    "    html_content += f\"<p>{summary}</p>\"\n",
    "    print(html_content)\n",
    "\n",
    "    return Response(html_content, mimetype='text/html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "    return set(filtered_words)\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calcule la similarité de Jaccard entre deux ensembles.\"\"\"\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def compare_texts(text1, text2):\n",
    "    set1 = preprocess_text(text1)\n",
    "    set2 = preprocess_text(text2)\n",
    "    return jaccard_similarity(set1, set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_comment(similarity):\n",
    "    if similarity <= 0.2:\n",
    "        return \"Les articles semblent très différents.\"\n",
    "    elif similarity <= 0.4:\n",
    "        return \"Les articles présentent quelques similarités, mais restent majoritairement différents.\"\n",
    "    elif similarity <= 0.6:\n",
    "        return \"Les articles sont modérément similaires.\"\n",
    "    elif similarity <= 0.8:\n",
    "        return \"Les articles semblent assez similaires.\"\n",
    "    else:\n",
    "        return \"Les articles semblent très ressemblants.\"\n",
    "\n",
    "@app.route('/ml/compare/<id1>/<id2>')\n",
    "def compare(id1, id2):\n",
    "    text1 = extractTEXTformPDF(id1)[0]\n",
    "    text2 = extractTEXTformPDF(id2)[0]\n",
    "    similarity = compare_texts(text1, text2)\n",
    "    comment = get_similarity_comment(similarity)\n",
    "    return f\"Similarité de Jaccard entre les deux textes: {similarity}. {comment}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [07/Apr/2024 18:50:23] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:50:28] \"GET /get_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:50:37] \"GET /articles HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:50:44] \"GET /article/2402.07632v1 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:50:57] \"GET /ml/2402.07632v1 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:51:00] \"GET /ml/keywords/2402.07632v1 HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:51:37] \"GET /keywords/2402.07632v1 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:53:37] \"GET /ml/2402.07632v1 HTTP/1.1\" 200 -\n",
      "[2024-04-07 18:53:40,740] ERROR in app: Exception on /ml/sum/2402.07632v1 [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\remil\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\remil\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\remil\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\remil\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "  File \"C:\\Users\\remil\\AppData\\Local\\Temp\\ipykernel_73940\\2063203738.py\", line 18, in auto_summary_article\n",
      "    summary = auto_summarize(text)\n",
      "  File \"C:\\Users\\remil\\AppData\\Local\\Temp\\ipykernel_73940\\2063203738.py\", line 2, in auto_summarize\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "  File \"C:\\Users\\remil\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\__init__.py\", line 51, in load\n",
      "    return util.load_model(\n",
      "  File \"C:\\Users\\remil\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\util.py\", line 472, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "127.0.0.1 - - [07/Apr/2024 18:53:40] \"GET /ml/sum/2402.07632v1 HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [07/Apr/2024 18:54:59] \"GET /ml/compare/2402.07632v1/2403.05551v1 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# faire tourner le serveur\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
